{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download('wordnet')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmetizer  = WordNetLemmatizer()\n",
    "with  open(\"train_ita_eng_spa_deu_fra.txt\",'r',encoding='utf-8') as fp:\n",
    "    language = []\n",
    "    language_text  = []\n",
    "    for line in fp:\n",
    "        splitted_value  = line.split(\"\\t\")\n",
    "        language.append(splitted_value[0].lower())\n",
    "        myText = splitted_value[1].rstrip().lower()\n",
    "        language_text.append(myText)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'while running campaign elect his sister sherry martschink lieutenant governor shealy with the help robert kohn recruited unemployed black fisherman benjamin hunt run for congress against republican arthur ravenel shealy sought increase the turnout white voter playing the racial fear the south carolina electorate promised hunt who had prior drug arrest run for congress and paid for his filing fee shealy paid for the fee with unreported campaign contribution his sister from laidlaw environmental service'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_text[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(corpus):\n",
    "    corpus_ = []\n",
    "    for sentence in corpus:\n",
    "        tr = str.maketrans(\"\",\"\",string.punctuation)\n",
    "        sentence =  sentence.translate(tr)\n",
    "        myCorpus = \"\"\n",
    "        for word in sentence.split(\" \"):\n",
    "            if not word.isdigit():\n",
    "                if len(word) > 2:\n",
    "                    \n",
    "                    myCorpus += lemmetizer.lemmatize(word)+\" \"\n",
    "        corpus_.append(myCorpus.rstrip())\n",
    "    \n",
    "    \n",
    "    return corpus_\n",
    "\n",
    "def bigram_words(splitted_sentence):\n",
    "    return nltk.bigrams(splitted_sentence)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_text  = preprocessing_text(language_text)\n",
    "language_text = pd.Series(language_text)\n",
    "language   =  pd.Series(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = np.stack([language_text,language],axis=1)\n",
    "df = pd.DataFrame(data=data,columns=[\"text\",\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value  in enumerate(set(df[\"label\"])):\n",
    "    \n",
    "    for name in range(len(df[\"label\"])):\n",
    "        if df.loc[name][\"label\"] ==value:\n",
    "            df.loc[name][\"label\"] = key\n",
    "              \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_division(text_data):\n",
    "    return list(nltk.bigrams(text_data))\n",
    "\n",
    "\n",
    "def bigram_to_index(bigram_data):\n",
    "    unique_bigram = set(bigram_data)\n",
    "    b_to_index  = {v:k for k,v in enumerate(unique_bigram)}\n",
    "    index_to_b  = {k:v for k,v in enumerate(unique_bigram)}\n",
    "    \n",
    "    return len(unique_bigram),b_to_index,index_to_b\n",
    "\n",
    "def generateX(myBigramData):\n",
    "    vector = np.zeros(unique_vocab_length)\n",
    "    #generate the frequency of the word\n",
    "    bigram_frequency = Counter(myBigramData)\n",
    "    \n",
    "    n_total = sum(bigram_frequency.values())\n",
    "    \n",
    "    for key,val in bigram_frequency.items():\n",
    "        vector[b_to_index[key]] = val/n_total\n",
    "        \n",
    "    return vector\n",
    "\n",
    "def tokenize(text_data):\n",
    "    tokenized_text   = keras.preprocessing.text.text_to_word_sequence(text_data)\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each row represent the vector and the vector is the unique value of the data \n",
    "text_data  = df[\"text\"].values\n",
    "text_data  = \" \".join(text_data)\n",
    "\n",
    "#how to find the unique value we need to divide the data into ngrams we can use the one hot encoding approach\n",
    "tokenized_text   = keras.preprocessing.text.text_to_word_sequence(text_data)\n",
    "\n",
    "bigram_data  = bigram_division(tokenized_text)\n",
    "unique_vocab_length , b_to_index,index_to_b = bigram_to_index(bigram_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 105776)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X  = []\n",
    "for sentence in df[\"text\"]:\n",
    "    sentence  =  tokenize(sentence)\n",
    "    big_ram = bigram_division(sentence)\n",
    "    X.append(generateX(big_ram))\n",
    "X  =  np.stack(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(Y)\n",
    "Y = Y.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_26 (Dense)             (None, 32)                3384864   \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 3,385,029\n",
      "Trainable params: 3,385,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#lets create a model \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.layers.Dense(32,input_shape=(X.shape[1],) ,activation='relu'))\n",
    "model.add(tf.layers.Dense(5,activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy')\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.6066\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 1s 554us/step - loss: 1.5643\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 1s 562us/step - loss: 1.4576\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 1s 550us/step - loss: 1.2723\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 1s 572us/step - loss: 1.0341\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 1s 564us/step - loss: 0.7828\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 1s 578us/step - loss: 0.5605\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 1s 561us/step - loss: 0.3904\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 1s 591us/step - loss: 0.2727\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 1s 568us/step - loss: 0.1949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x3fff33e31780>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,Y,epochs=10,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1er avril le service asama sont actuellement effectués par de shinkansen série il \n",
    "étaient également effectués par \n",
    "de shinkansen série\n",
    "'''\n",
    "\n",
    "sentence =  [\"1er avril le service asama sont actuellement effectués par de shinkansen série il étaient également effectués par de shinkansen série\",\"while running campaign elect his sister sherry martschink lieutenant\"]\n",
    "X = []\n",
    "for sent in sentence:\n",
    "    sentence  =  tokenize(sent)\n",
    "    big_ram = bigram_division(sentence)\n",
    "    X.append(generateX(big_ram))\n",
    "\n",
    "X = np.stack(X)\n",
    "\n",
    "predicted_value  = model.predict(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fra\n",
      "eng\n"
     ]
    }
   ],
   "source": [
    "output = {k:v for k,v in enumerate(set(language))}\n",
    "\n",
    "for row in range(len(predicted_value)):\n",
    "    print(output[np.argmax(predicted_value[row,:])])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

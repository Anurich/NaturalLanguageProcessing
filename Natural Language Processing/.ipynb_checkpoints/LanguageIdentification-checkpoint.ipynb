{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download('wordnet')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmetizer  = WordNetLemmatizer()\n",
    "with  open(\"train_ita_eng_spa_deu_fra.txt\",'r',encoding='utf-8') as fp:\n",
    "    language = []\n",
    "    language_text  = []\n",
    "    for line in fp:\n",
    "        splitted_value  = line.split(\"\\t\")\n",
    "        language.append(splitted_value[0].lower())\n",
    "        myText = splitted_value[1].rstrip().lower()\n",
    "        language_text.append(myText)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(corpus):\n",
    "    corpus_ = []\n",
    "    for sentence in corpus:\n",
    "        tr = str.maketrans(\"\",\"\",string.punctuation)\n",
    "        sentence =  sentence.translate(tr)\n",
    "        myCorpus = \"\"\n",
    "        for word in sentence.split(\" \"):\n",
    "            if not word.isdigit():\n",
    "                if len(word) > 2:\n",
    "                    \n",
    "                    myCorpus += lemmetizer.lemmatize(word)+\" \"\n",
    "        corpus_.append(myCorpus.rstrip())\n",
    "    \n",
    "    \n",
    "    return corpus_\n",
    "\n",
    "def bigram_words(splitted_sentence):\n",
    "    return nltk.bigrams(splitted_sentence)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_text  = preprocessing_text(language_text)\n",
    "language_text = pd.Series(language_text)\n",
    "language   =  pd.Series(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = np.stack([language_text,language],axis=1)\n",
    "df = pd.DataFrame(data=data,columns=[\"text\",\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value  in enumerate(set(df[\"label\"])):\n",
    "    \n",
    "    for name in range(len(df[\"label\"])):\n",
    "        if df.loc[name][\"label\"] ==value:\n",
    "            df.loc[name][\"label\"] = key\n",
    "              \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_division(text_data):\n",
    "    return list(nltk.bigrams(text_data))\n",
    "\n",
    "\n",
    "def bigram_to_index(bigram_data):\n",
    "    unique_bigram = set(bigram_data)\n",
    "    b_to_index  = {v:k for k,v in enumerate(unique_bigram)}\n",
    "    index_to_b  = {k:v for k,v in enumerate(unique_bigram)}\n",
    "    \n",
    "    return len(unique_bigram),b_to_index,index_to_b\n",
    "\n",
    "def generateX(myBigramData):\n",
    "    vector = np.zeros(unique_vocab_length)\n",
    "    #generate the frequency of the word\n",
    "    bigram_frequency = Counter(myBigramData)\n",
    "    \n",
    "    n_total = sum(bigram_frequency.values())\n",
    "    X = []\n",
    "    for key,val in bigram_frequency.items():\n",
    "        vector[b_to_index[key]] = val/n_total\n",
    "    X.append(vector)\n",
    "    return np.stack(X)\n",
    "\n",
    "def tokenize():\n",
    "    tokenized_text   = keras.preprocessing.text.text_to_word_sequence(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each row represent the vector and the vector is the unique value of the data \n",
    "text_data  = df[\"text\"].values\n",
    "text_data  = \" \".join(text_data)\n",
    "\n",
    "#how to find the unique value we need to divide the data into ngrams we can use the one hot encoding approach\n",
    "tokenized_text   = keras.preprocessing.text.text_to_word_sequence(text_data)\n",
    "\n",
    "bigram_data  = bigram_division(tokenized_text)\n",
    "unique_vocab_length , b_to_index,index_to_b = bigram_to_index(bigram_data[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 's'), ('s', 's'), ('s', 'o'), ('o', 'c'), ('c', 'i'), ('i', 'a'), ('a', 't'), ('t', 'i'), ('i', 'o'), ('o', 'n'), ('n', ' '), (' ', 'r'), ('r', 'e'), ('e', 'c'), ('c', 'h'), ('h', 'e'), ('e', 'r'), ('r', 'c'), ('c', 'h'), ('h', 'e'), ('e', ' '), (' ', 's'), ('s', 'a'), ('a', 'u'), ('u', 'v'), ('v', 'e'), ('e', 'g'), ('g', 'a'), ('a', 'r'), ('r', 'd'), ('d', 'e'), ('e', ' '), (' ', 'l'), ('l', 'h'), ('h', 'i'), ('i', 's'), ('s', 't'), ('t', 'o'), ('o', 'i'), ('i', 'r'), ('r', 'e'), ('e', ' '), (' ', 'r'), ('r', 'o'), ('o', 'i'), ('i', 's'), ('s', 's'), ('s', 'y'), ('y', 'e'), ('e', 'n'), ('n', 'f'), ('f', 'r'), ('r', 'a'), ('a', 'n'), ('n', 'c'), ('c', 'e'), ('e', ' '), (' ', 'a'), ('a', 'r'), ('r', 's'), ('s', 'h'), ('h', 'r'), ('r', 'f'), ('f', ' '), (' ', 'r'), ('r', 'o'), ('o', 'i'), ('i', 's'), ('s', 's'), ('s', 'y'), ('y', 'e'), ('e', 'n'), ('n', 'f'), ('f', 'r'), ('r', 'a'), ('a', 'n'), ('n', 'c'), ('c', 'e'), ('e', ' '), (' ', 'm'), ('m', 'é'), ('é', 'm'), ('m', 'o'), ('o', 'i'), ('i', 'r'), ('r', 'e'), ('e', ' '), (' ', 'd'), ('d', 'u'), ('u', 'n'), ('n', ' '), (' ', 's'), ('s', 'i'), ('i', 'è'), ('è', 'c'), ('c', 'l'), ('l', 'e'), ('e', ' '), (' ', 'v'), ('v', 'o'), ('o', 'i'), ('i', 'r'), ('r', ' '), (' ', 'd'), ('d', 'a'), ('a', 'n'), ('n', 's'), ('s', ' '), (' ', 'b'), ('b', 'i'), ('i', 'b'), ('b', 'l'), ('l', 'i'), ('i', 'o'), ('o', 'g'), ('g', 'r'), ('r', 'a'), ('a', 'p'), ('p', 'h'), ('h', 'i'), ('i', 'e')]\n"
     ]
    }
   ],
   "source": [
    "for sentence in df[\"text\"]:\n",
    "    big_ram = bigram_division(sentence)\n",
    "    print(big_ram)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
